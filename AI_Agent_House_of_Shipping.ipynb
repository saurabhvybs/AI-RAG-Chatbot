{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhvybs/AI-RAG-Chatbot/blob/main/AI_Agent_House_of_Shipping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk6g6S9vXaLV"
      },
      "source": [
        "## HOUSE OF SHIPPING AGENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MpuBNouqZZTg"
      },
      "outputs": [],
      "source": [
        "# All Installations\n",
        "!pip install \\\n",
        "    langchain langchain-community langchain-huggingface langchain-google-genai langchain-chroma \\\n",
        "    chromadb sentence-transformers \\\n",
        "    selenium beautifulsoup4 \\\n",
        "    pandas scikit-learn seaborn matplotlib \\\n",
        "    tiktoken flashrank -q\n",
        "\n",
        "# Install the browser and its driver for Selenium in Colab\n",
        "!apt-get update -qq\n",
        "!apt-get install -y chromium-browser chromium-chromedriver -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RQpOw0Vo0dcc"
      },
      "outputs": [],
      "source": [
        "!pip uninstall google-generativeai google-ai-generativelanguage -y\n",
        "!pip install langchain-google-genai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRAPER SERVICE (Selenium)"
      ],
      "metadata": {
        "id": "Q24jWBtQGF0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LpGT3ZAv2KO-"
      },
      "outputs": [],
      "source": [
        "# Scraper Service\n",
        "import sys\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# --- Setup for Colab ---\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "# ----------------------\n",
        "\n",
        "def scrape_website_for_structured_html(start_url: str) -> dict:\n",
        "    \"\"\"\n",
        "    This definitive scraper captures the cleaned HTML structure from the main content\n",
        "    area of each page, preserving headers for advanced chunking.\n",
        "    \"\"\"\n",
        "    to_visit = {start_url}\n",
        "    visited = set()\n",
        "    scraped_data = {}\n",
        "    base_netloc = urlparse(start_url).netloc\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    print(f\"ðŸš€ Starting structured scrape at: {start_url}\")\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop()\n",
        "        if current_url in visited:\n",
        "            continue\n",
        "        try:\n",
        "            print(f\"Scraping: {current_url}\")\n",
        "            driver.get(current_url)\n",
        "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
        "\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "            visited.add(current_url)\n",
        "\n",
        "            # Find the main content area to avoid irrelevant headers/footers\n",
        "            content_area = soup.find('main') if soup.find('main') else soup.find('body')\n",
        "\n",
        "            # **Crucially, we clean *within* the HTML, keeping header tags intact**\n",
        "            if content_area:\n",
        "                for element in content_area(['script', 'style', 'nav', 'footer', 'aside']):\n",
        "                    element.decompose() # Remove noise, but keep h1, h2, p, etc.\n",
        "\n",
        "            scraped_data[current_url] = str(content_area) if content_area else \"\"\n",
        "\n",
        "            # Crawling logic remains the same\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                # ... (rest of link finding logic is unchanged)\n",
        "                absolute_link = urljoin(current_url, link['href'])\n",
        "                parsed_link = urlparse(absolute_link)\n",
        "                if (parsed_link.netloc == base_netloc and\n",
        "                    parsed_link.scheme in ['http', 'https'] and\n",
        "                    absolute_link not in visited and\n",
        "                    \"#\" not in absolute_link.split('/')[-1]):\n",
        "                    to_visit.add(absolute_link)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process {current_url}: {e}\")\n",
        "\n",
        "    driver.quit()\n",
        "    print(f\"\\n Scraping complete. Found {len(scraped_data)} pages.\")\n",
        "    return scraped_data\n",
        "\n",
        "# --- Main Execution ---\n",
        "website_url = \"https://houseofshipping.com\"\n",
        "structured_content = scrape_website_for_structured_html(website_url)\n",
        "\n",
        "output_filename = \"structured_web_content.json\"\n",
        "with open(output_filename, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(structured_content, f, indent=4, ensure_ascii=False)\n",
        "print(f\" Structured HTML content saved to '{output_filename}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fl_YEgCcVS6"
      },
      "source": [
        "# EDA (Exploratory Data Analysis) N-gram Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrpSGDJ23qmR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load Structured Data & Perform Full EDA\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- 1. Load the Scraped Data ---\n",
        "json_file_path = 'structured_web_content.json'\n",
        "try:\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.DataFrame(list(data.items()), columns=['url', 'raw_html'])\n",
        "    print(f\" Successfully loaded {len(df)} structured documents from '{json_file_path}'.\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: Could not load the file. Please ensure the scraper in Cell 1 has been run successfully. Error: {e}\")\n",
        "\n",
        "\n",
        "# --- 2. For EDA, extract and clean plain text from HTML ---\n",
        "# This is for analysis only; the raw_html is preserved for chunking.\n",
        "df['text_for_eda'] = df['raw_html'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text(separator=' ', strip=True).lower())\n",
        "\n",
        "\n",
        "# --- 3. Advanced EDA: Find and Visualize Top N-grams ---\n",
        "print(\"--- Analyzing Top 2-Word and 3-Word Phrases ---\")\n",
        "\n",
        "def get_top_ngrams(corpus, n=None, ngram_range=(1, 1)):\n",
        "    \"\"\"Extracts top n-grams from a text corpus.\"\"\"\n",
        "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Get top 20 bigrams (2-word phrases)\n",
        "top_bigrams = get_top_ngrams(df['text_for_eda'], n=20, ngram_range=(2, 2))\n",
        "bigram_df = pd.DataFrame(top_bigrams, columns=['phrase', 'count'])\n",
        "\n",
        "# Get top 20 trigrams (3-word phrases)\n",
        "top_trigrams = get_top_ngrams(df['text_for_eda'], n=20, ngram_range=(3, 3))\n",
        "trigram_df = pd.DataFrame(top_trigrams, columns=['phrase', 'count'])\n",
        "\n",
        "\n",
        "# --- 4. Visualize the Results ---\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Bigrams plot\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x='count', y='phrase', data=bigram_df, palette='viridis', hue='phrase', legend=False)\n",
        "plt.title('Top 20 Most Common 2-Word Phrases', fontsize=16)\n",
        "plt.xlabel(\"Frequency\", fontsize=12)\n",
        "plt.ylabel(\"Phrase\", fontsize=12)\n",
        "\n",
        "\n",
        "# Trigrams plot\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x='count', y='phrase', data=trigram_df, palette='plasma', hue='phrase', legend=False)\n",
        "plt.title('Top 20 Most Common 3-Word Phrases', fontsize=16)\n",
        "plt.xlabel(\"Frequency\", fontsize=12)\n",
        "plt.ylabel(\"\") # Hide y-label for cleaner look\n",
        "\n",
        "plt.suptitle(\"Advanced EDA: Key Phrase Analysis\", fontsize=20, weight='bold')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MANUAL Q&A PAIR FEEDING\n",
        "## For Q&A , for which you don't have specific data."
      ],
      "metadata": {
        "id": "b65xh6xHGW8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string with our high-quality, manual Q&A pairs\n",
        "manual_qa_content = \"\"\"\n",
        "# What specific IT services are offered by House of Shipping?\n",
        "\n",
        "House of Shipping provides a comprehensive suite of IT services tailored for the logistics and shipping industry, including:\n",
        "- Custom freight management software development.\n",
        "- Cybersecurity audits and solutions for logistics networks.\n",
        "- Cloud migration and infrastructure management (AWS, Azure).\n",
        "- Data analytics platforms and predictive maintenance dashboards for fleets.\n",
        "- Implementation of digital twin technologies for supply chain optimization.\n",
        "\n",
        "# What specific legal services does the company provide?\n",
        "\n",
        "The legal team at House of Shipping offers several key services to ensure compliance and manage risk:\n",
        "- Company Incorporation: Assisting businesses in establishing legal entities and streamlining corporate structures.\n",
        "- Legal Operations Optimization: Refining legal models to align with business goals.\n",
        "- Compliance and Risk Management: Developing internal audit and corporate compliance programs.\n",
        "- Strategic Legal Guidance: Helping to develop and monitor a groupâ€™s legal strategy.\n",
        "- Claims Handling: Covering a wide array of needs, leveraging deep knowledge of the maritime and logistics sectors.\n",
        "\n",
        "# How did House of Shipping assist WeFreight with their expansion?\n",
        "\n",
        "House of Shipping's marketing and HR teams acted as a strategic partner for WeFreight's global expansion. This included developing a global talent acquisition strategy to quickly staff new international offices and creating targeted marketing campaigns to establish a brand presence in new markets. The direct result was WeFreight's swift and successful launch in multiple new countries, including Mexico, and their recognition as a \"Great Place to Work.\"\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to a new file\n",
        "with open(\"manual_qa.txt\", \"w\") as f:\n",
        "    f.write(manual_qa_content)\n",
        "\n",
        "print(\"âœ… Comprehensive manual Q&A file 'manual_qa.txt' created successfully.\")"
      ],
      "metadata": {
        "id": "xuiNEWyI7L0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load New Knowledge and create data frames"
      ],
      "metadata": {
        "id": "3abT2sSSG6QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the content from our new manual Q&A file\n",
        "with open(\"manual_qa.txt\", \"r\") as f:\n",
        "    new_knowledge = f.read()\n",
        "\n",
        "# Create a new row for the DataFrame\n",
        "new_row = pd.DataFrame([{\n",
        "    \"url\": \"internal_document_qa.txt\",\n",
        "    \"raw_html\": f\"<html><body>{new_knowledge}</body></html>\", # Wrap in basic HTML for the splitter\n",
        "    \"text_for_eda\": new_knowledge\n",
        "}])\n",
        "\n",
        "# Add this new knowledge to our main DataFrame\n",
        "df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "print(f\"âœ… New Q&A knowledge added. Total documents now: {len(df)}\")"
      ],
      "metadata": {
        "id": "oj90yZis7yIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OofEdxbQzQWG"
      },
      "source": [
        "# DATA Preparation and Advanced Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uG6pDjST4dyD"
      },
      "outputs": [],
      "source": [
        "# Structural Chunking and Templating\n",
        "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 1. Define the primary splitter that understands HTML structure\n",
        "headers_to_split_on = [\n",
        "    (\"h1\", \"H1\"),\n",
        "    (\"h2\", \"H2\"),\n",
        "    (\"h3\", \"H3\"),\n",
        "    (\"h4\", \"H4\"),\n",
        "]\n",
        "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on, return_each_element=False)\n",
        "\n",
        "# 2. Define a fallback splitter for any large chunks of text without headers\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "# 3. Process the data from the DataFrame\n",
        "all_documents = []\n",
        "for index, row in df.iterrows():\n",
        "    # Ensure the content is a string\n",
        "    if not isinstance(row['raw_html'], str) or not row['raw_html'].strip():\n",
        "        continue\n",
        "\n",
        "    # First, try to split by headers.\n",
        "    header_chunks = html_splitter.split_text(row['raw_html'])\n",
        "\n",
        "    # Now, iterate through these chunks and split any that are too large\n",
        "    for chunk in header_chunks:\n",
        "        # Clean up the text content within the chunk\n",
        "        chunk_text = BeautifulSoup(chunk.page_content, 'html.parser').get_text(separator=' ', strip=True)\n",
        "\n",
        "        # Skip chunks with very little actual text\n",
        "        if len(chunk_text.split()) < 10:\n",
        "            continue\n",
        "\n",
        "        if len(chunk_text) > 1000:\n",
        "            # If a chunk is too big, split it recursively but keep its header metadata\n",
        "            sub_chunks = recursive_splitter.create_documents([chunk_text])\n",
        "            for sub_chunk in sub_chunks:\n",
        "                # Add the original header metadata to the new sub-chunk\n",
        "                sub_chunk.metadata = chunk.metadata.copy()\n",
        "                sub_chunk.metadata['source'] = row['url']\n",
        "                all_documents.append(sub_chunk)\n",
        "        else:\n",
        "            # If the chunk is a good size, just update its content and metadata\n",
        "            chunk.page_content = chunk_text\n",
        "            chunk.metadata['source'] = row['url']\n",
        "            all_documents.append(chunk)\n",
        "\n",
        "print(f\" Successfully created {len(all_documents)} structurally-aware documents (chunks).\")\n",
        "\n",
        "# Display a sample chunk to see the result\n",
        "if all_documents:\n",
        "    print(\"\\n--- SAMPLE CHUNK ---\")\n",
        "    sample_chunk = all_documents[5] # Using a different index for variety\n",
        "    print(sample_chunk.page_content)\n",
        "    print(f\"\\nMetadata: {sample_chunk.metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKsvEGKd6N-K"
      },
      "source": [
        "# Vector Knowledge base for RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "drqGBoia6TmB"
      },
      "outputs": [],
      "source": [
        "# Build RAG Knowledge Base (Vector Store)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document # Included for clarity\n",
        "\n",
        "print(\"\\n--- Building RAG Knowledge Base ---\")\n",
        "\n",
        "# 1. Load the Embedding Model\n",
        "# This model will convert your text chunks into numerical vectors.\n",
        "print(\"Loading embedding model (this may take a minute)...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'} # Use CPU for broad compatibility\n",
        ")\n",
        "print(\" Embedding model loaded.\")\n",
        "\n",
        "\n",
        "# 2. Create the Chroma Vector Store from the documents\n",
        "# This is the step where your chunks are embedded and stored.\n",
        "print(\"Storing chunks in the vector database...\")\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=all_documents,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./final_chroma_db\"  # This saves the database to a local folder for reuse\n",
        ")\n",
        "print(\" Vector store created.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- RAG Data Preparation Complete! ---\")\n",
        "print(\"The 'vector_store' variable now holds your complete, searchable knowledge base.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofN8yfhCAS9p"
      },
      "source": [
        "# Adding GEMNI as the Powerful Generalist Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arEzZe3ECqbC"
      },
      "outputs": [],
      "source": [
        "# Setup for Google Gemini\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\" Google API Key configured successfully.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\" Error: Secret 'GOOGLE_API_KEY' not found. Please add it to your Colab secrets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP7PG74hDxye"
      },
      "source": [
        "# Loading the Knowledge back into the run-time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xHgLNvySC9rT"
      },
      "outputs": [],
      "source": [
        "# Load the Existing Vector Database\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "print(\"--- Loading the Knowledge Base from Disk ---\")\n",
        "\n",
        "#  Initialize the same embedding model used to create the database\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "# 2. Load the saved vector store from the specified directory\n",
        "persist_directory = \"./final_chroma_db\"\n",
        "vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "print(\" Knowledge Base loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGRMrloIGq1w"
      },
      "source": [
        "# Define and Build RAG Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0cIgd6TIblp"
      },
      "source": [
        "## Prompt Stuffing File."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80rMIItbQtQK"
      },
      "outputs": [],
      "source": [
        "#  Create the Structured JSON Prompt Files\n",
        "import json\n",
        "\n",
        "# 1. Define the prompt structure for the history-aware retriever\n",
        "contextualize_q_prompt_data = [\n",
        "    (\"system\", \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"),\n",
        "    (\"placeholder\", \"{chat_history}\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "]\n",
        "\n",
        "# 2. Define the prompt structure for the final QA chain\n",
        "qa_prompt_data = [\n",
        "    [\n",
        "        \"system\",\n",
        "        \"You are an expert assistant for the House of Shipping company. Your tone should be professional and helpful. Answer the user's latest question using both the chat history and the retrieved context.\\n\\n**Rules:**\\n- Synthesize information from both the 'Retrieved Context' and the 'Chat History' to form a complete, conversational answer.\\n- If the 'Retrieved Context' does not contain information to answer the latest question, you MUST state: \\\"I do not have enough information to answer that specific question.\\\"\\n- Do not contradict information that has already been established in the 'Chat History'.\\n\\n**Retrieved Context:**\\n{context}\\n\"\n",
        "    ],\n",
        "    [\n",
        "        \"placeholder\",\n",
        "        \"{chat_history}\"\n",
        "    ],\n",
        "    [\n",
        "        \"human\",\n",
        "        \"{input}\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "# 3. Write the structures to JSON files\n",
        "with open(\"contextualize_prompt.json\", \"w\") as f:\n",
        "    json.dump(contextualize_q_prompt_data, f, indent=2)\n",
        "\n",
        "with open(\"qa_prompt.json\", \"w\") as f:\n",
        "    json.dump(qa_prompt_data, f, indent=2)\n",
        "\n",
        "print(\"âœ… Structured prompt files 'contextualize_prompt.json' and 'qa_prompt.json' created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWBFqTPxQ7Bt"
      },
      "source": [
        "# History-Aware Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svrIvNPdT2JO"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "print(\"âœ… Chat history object initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Advance Retriever for Strict Answers from VectorDb."
      ],
      "metadata": {
        "id": "JaUsRMXEHe-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnKwKQSvT-GD"
      },
      "outputs": [],
      "source": [
        "#  Define the Re-ranking Retriever\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import FlashrankRerank\n",
        "\n",
        "# 1. Define the LLM and Helper Function\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flask\")\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 2. Create the BASE retriever from your vector store.\n",
        "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# 3. Instantiate the FlashrankRerank compressor.\n",
        "#    It's simpler and automatically uses a fast, effective model.\n",
        "compressor = FlashrankRerank()\n",
        "\n",
        "# 4. Create the final Contextual Compression Retriever.\n",
        "retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=base_retriever\n",
        ")\n",
        "\n",
        "print(\" Advanced re-ranking retriever created using Flashrank.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Conversational History Aware Retriever."
      ],
      "metadata": {
        "id": "e0syvMZYH0L2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ3M3p0vUHTI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Load the structured prompt from its JSON file\n",
        "with open(\"contextualize_prompt.json\", \"r\") as f:\n",
        "    contextualize_q_prompt_data = json.load(f)\n",
        "\n",
        "\n",
        "# Manually build the list of messages, creating an explicit MessagesPlaceholder object.\n",
        "messages = []\n",
        "for role, content in contextualize_q_prompt_data:\n",
        "    if role == \"placeholder\":\n",
        "        # Extract the variable name from inside the curly braces (e.g., \"{chat_history}\")\n",
        "        variable_name = content.strip('{}')\n",
        "        messages.append(MessagesPlaceholder(variable_name=variable_name))\n",
        "    else:\n",
        "        messages.append((role, content))\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "# --------------------\n",
        "\n",
        "# Chaining\n",
        "history_aware_retriever_chain = (\n",
        "    contextualize_q_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | retriever\n",
        ")\n",
        "\n",
        "print(\" History-aware retriever chain created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final RAG Engine and Chaining."
      ],
      "metadata": {
        "id": "7wLLRHhMIEG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogm8fncoUsMj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Load the structured prompt for the final answer from its JSON file\n",
        "with open(\"qa_prompt.json\", \"r\") as f:\n",
        "    qa_prompt_data = json.load(f)\n",
        "\n",
        "# Manually build the list of messages, creating an explicit MessagesPlaceholder object.\n",
        "messages = []\n",
        "for item in qa_prompt_data:\n",
        "    role, content = item\n",
        "    if role == \"placeholder\":\n",
        "        variable_name = content.strip('{}')\n",
        "        messages.append(MessagesPlaceholder(variable_name=variable_name))\n",
        "    else:\n",
        "        messages.append((role, content))\n",
        "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "\n",
        "def retrieve_and_format_context(inputs):\n",
        "    # First, invoke the history-aware retriever to get the documents\n",
        "    docs = history_aware_retriever_chain.invoke({\n",
        "        \"input\": inputs[\"input\"],\n",
        "        \"chat_history\": inputs[\"chat_history\"]\n",
        "    })\n",
        "    # Then, format the documents into a single string\n",
        "    return format_docs(docs)\n",
        "# --------------------\n",
        "\n",
        "\n",
        "# Build the final conversational RAG chain using the corrected logic\n",
        "conversational_rag_chain = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"context\": retrieve_and_format_context\n",
        "    }\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\" Final conversational RAG chain is built and ready for testing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blcil9lfKHJ2"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UwQ5dMFU7nh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the test suite as a list of test cases\n",
        "test_suite = [\n",
        "    {\n",
        "        \"name\": \"Fact Retrieval about Services\",\n",
        "        \"type\": \"Knowledge\",\n",
        "        \"steps\": [\"What kind of IT services are offered by House of Shipping?\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Conceptual Question from Insights\",\n",
        "        \"type\": \"Knowledge\",\n",
        "        \"steps\": [\"Based on your articles, what is the importance of digital transformation for the shipping industry?\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Rule-Following - Out of Scope Question\",\n",
        "        \"type\": \"Rule-Following\",\n",
        "        \"steps\": [f\"What is the current time in Lucknow? It is currently {datetime.now().strftime('%I:%M:%S %p')}.\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Conversational Memory - Simple Follow-up\",\n",
        "        \"type\": \"Memory\",\n",
        "        \"steps\": [\n",
        "            \"How did House of Shipping assist WeFreight with their expansion?\",\n",
        "            \"What was the direct result of that assistance?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Conversational Memory - Topic Change\",\n",
        "        \"type\": \"Memory\",\n",
        "        \"steps\": [\n",
        "            \"Tell me about the company's legal services.\",\n",
        "            \"Does that also include any marketing services?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Rule-Following - Plausible but Unavailable Detail\",\n",
        "        \"type\": \"Rule-Following\",\n",
        "        \"steps\": [\"Can you give me the direct phone number for the Global CEO, Alessandra Ronco?\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Test Runner ---\n",
        "print(\" STARTING COMPREHENSIVE AGENT TEST SUITE \")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, test in enumerate(test_suite):\n",
        "    print(f\"\\n EXECUTING TEST {i+1}: {test['name']} ({test['type']})\")\n",
        "    print(\"-\"*40)\n",
        "\n",
        "    # Initialize a fresh, independent chat history for each test case\n",
        "    test_chat_history = ChatMessageHistory()\n",
        "\n",
        "    # Loop through the steps (questions) for the current test\n",
        "    for step, question in enumerate(test['steps']):\n",
        "        print(f\"\\nStep {step + 1} of {len(test['steps'])}\")\n",
        "        print(f\" You: {question}\")\n",
        "\n",
        "        # Invoke the conversational chain with the current question and the test's history\n",
        "        answer = conversational_rag_chain.invoke({\n",
        "            \"input\": question,\n",
        "            \"chat_history\": test_chat_history.messages\n",
        "        })\n",
        "\n",
        "        print(f\"\\nðŸ¤– AI: {answer}\")\n",
        "\n",
        "        # Update the test's chat history for the next step\n",
        "        test_chat_history.add_user_message(question)\n",
        "        test_chat_history.add_ai_message(answer)\n",
        "\n",
        "\n",
        "        # Pause for 20 seconds after every single prompt to respect API rate limits.\n",
        "        if step < len(test['steps']) - 1: # Optional: prevents waiting after the last step of a test\n",
        "            print(\"\\n Pausing for 20 seconds...\")\n",
        "            time.sleep(20)\n",
        "        # --------------------\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\n TEST SUITE COMPLETE,,, Shakalaka BOOM BOOM \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmo3aaqGKV6D"
      },
      "source": [
        "## Now Improve the Prompt Rules for Handling User more Efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QImmBUaOKpo3"
      },
      "source": [
        "## And If needed simply Ingest more DATA to it and keep the Agent Up-to Date or simply add cron jobs to scrape the site after specific time Interval."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdtAgChqGVBbO6dl+JFGkx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}