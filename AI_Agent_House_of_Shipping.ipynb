{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmmOXRQVBIdUsKTqHUcWGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhvybs/AI-RAG-Chatbot/blob/main/AI_Agent_House_of_Shipping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HOUSE OF SHIPPING AGENT"
      ],
      "metadata": {
        "id": "Vk6g6S9vXaLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Python libraries\n",
        "!pip install selenium beautifulsoup4\n",
        "\n",
        "# Install the browser and its driver\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-browser chromium-chromedriver"
      ],
      "metadata": {
        "id": "MpuBNouqZZTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraper Service\n",
        "import sys\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# --- Setup for Colab ---\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "# ----------------------\n",
        "\n",
        "def scrape_website_for_structured_html(start_url: str) -> dict:\n",
        "    \"\"\"\n",
        "    This definitive scraper captures the cleaned HTML structure from the main content\n",
        "    area of each page, preserving headers for advanced chunking.\n",
        "    \"\"\"\n",
        "    to_visit = {start_url}\n",
        "    visited = set()\n",
        "    scraped_data = {}\n",
        "    base_netloc = urlparse(start_url).netloc\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    print(f\"ðŸš€ Starting structured scrape at: {start_url}\")\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop()\n",
        "        if current_url in visited:\n",
        "            continue\n",
        "        try:\n",
        "            print(f\"Scraping: {current_url}\")\n",
        "            driver.get(current_url)\n",
        "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
        "\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "            visited.add(current_url)\n",
        "\n",
        "            # Find the main content area to avoid irrelevant headers/footers\n",
        "            content_area = soup.find('main') if soup.find('main') else soup.find('body')\n",
        "\n",
        "            # **Crucially, we clean *within* the HTML, keeping header tags intact**\n",
        "            if content_area:\n",
        "                for element in content_area(['script', 'style', 'nav', 'footer', 'aside']):\n",
        "                    element.decompose() # Remove noise, but keep h1, h2, p, etc.\n",
        "\n",
        "            scraped_data[current_url] = str(content_area) if content_area else \"\"\n",
        "\n",
        "            # Crawling logic remains the same\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                # ... (rest of link finding logic is unchanged)\n",
        "                absolute_link = urljoin(current_url, link['href'])\n",
        "                parsed_link = urlparse(absolute_link)\n",
        "                if (parsed_link.netloc == base_netloc and\n",
        "                    parsed_link.scheme in ['http', 'https'] and\n",
        "                    absolute_link not in visited and\n",
        "                    \"#\" not in absolute_link.split('/')[-1]):\n",
        "                    to_visit.add(absolute_link)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process {current_url}: {e}\")\n",
        "\n",
        "    driver.quit()\n",
        "    print(f\"\\n Scraping complete. Found {len(scraped_data)} pages.\")\n",
        "    return scraped_data\n",
        "\n",
        "# --- Main Execution ---\n",
        "website_url = \"https://houseofshipping.com\"\n",
        "structured_content = scrape_website_for_structured_html(website_url)\n",
        "\n",
        "output_filename = \"structured_web_content.json\"\n",
        "with open(output_filename, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(structured_content, f, indent=4, ensure_ascii=False)\n",
        "print(f\" Structured HTML content saved to '{output_filename}'\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LpGT3ZAv2KO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA (Exploratory Data Analysis) N-gram Analysis"
      ],
      "metadata": {
        "id": "0Fl_YEgCcVS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Structured Data & Perform Full EDA\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- 1. Load the Scraped Data ---\n",
        "json_file_path = 'structured_web_content.json'\n",
        "try:\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.DataFrame(list(data.items()), columns=['url', 'raw_html'])\n",
        "    print(f\" Successfully loaded {len(df)} structured documents from '{json_file_path}'.\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: Could not load the file. Please ensure the scraper in Cell 1 has been run successfully. Error: {e}\")\n",
        "\n",
        "\n",
        "# --- 2. For EDA, extract and clean plain text from HTML ---\n",
        "# This is for analysis only; the raw_html is preserved for chunking.\n",
        "df['text_for_eda'] = df['raw_html'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text(separator=' ', strip=True).lower())\n",
        "\n",
        "\n",
        "# --- 3. Advanced EDA: Find and Visualize Top N-grams ---\n",
        "print(\"--- Analyzing Top 2-Word and 3-Word Phrases ---\")\n",
        "\n",
        "def get_top_ngrams(corpus, n=None, ngram_range=(1, 1)):\n",
        "    \"\"\"Extracts top n-grams from a text corpus.\"\"\"\n",
        "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Get top 20 bigrams (2-word phrases)\n",
        "top_bigrams = get_top_ngrams(df['text_for_eda'], n=20, ngram_range=(2, 2))\n",
        "bigram_df = pd.DataFrame(top_bigrams, columns=['phrase', 'count'])\n",
        "\n",
        "# Get top 20 trigrams (3-word phrases)\n",
        "top_trigrams = get_top_ngrams(df['text_for_eda'], n=20, ngram_range=(3, 3))\n",
        "trigram_df = pd.DataFrame(top_trigrams, columns=['phrase', 'count'])\n",
        "\n",
        "\n",
        "# --- 4. Visualize the Results ---\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Bigrams plot\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x='count', y='phrase', data=bigram_df, palette='viridis', hue='phrase', legend=False)\n",
        "plt.title('Top 20 Most Common 2-Word Phrases', fontsize=16)\n",
        "plt.xlabel(\"Frequency\", fontsize=12)\n",
        "plt.ylabel(\"Phrase\", fontsize=12)\n",
        "\n",
        "\n",
        "# Trigrams plot\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x='count', y='phrase', data=trigram_df, palette='plasma', hue='phrase', legend=False)\n",
        "plt.title('Top 20 Most Common 3-Word Phrases', fontsize=16)\n",
        "plt.xlabel(\"Frequency\", fontsize=12)\n",
        "plt.ylabel(\"\") # Hide y-label for cleaner look\n",
        "\n",
        "plt.suptitle(\"Advanced EDA: Key Phrase Analysis\", fontsize=20, weight='bold')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CrpSGDJ23qmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA Preparation and Advanced Cleaning"
      ],
      "metadata": {
        "id": "OofEdxbQzQWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Structural Chunking and Templating\n",
        "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 1. Define the primary splitter that understands HTML structure\n",
        "headers_to_split_on = [\n",
        "    (\"h1\", \"H1\"),\n",
        "    (\"h2\", \"H2\"),\n",
        "    (\"h3\", \"H3\"),\n",
        "    (\"h4\", \"H4\"),\n",
        "]\n",
        "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on, return_each_element=False)\n",
        "\n",
        "# 2. Define a fallback splitter for any large chunks of text without headers\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "# 3. Process the data from the DataFrame\n",
        "all_documents = []\n",
        "for index, row in df.iterrows():\n",
        "    # Ensure the content is a string\n",
        "    if not isinstance(row['raw_html'], str) or not row['raw_html'].strip():\n",
        "        continue\n",
        "\n",
        "    # First, try to split by headers.\n",
        "    header_chunks = html_splitter.split_text(row['raw_html'])\n",
        "\n",
        "    # Now, iterate through these chunks and split any that are too large\n",
        "    for chunk in header_chunks:\n",
        "        # Clean up the text content within the chunk\n",
        "        chunk_text = BeautifulSoup(chunk.page_content, 'html.parser').get_text(separator=' ', strip=True)\n",
        "\n",
        "        # Skip chunks with very little actual text\n",
        "        if len(chunk_text.split()) < 10:\n",
        "            continue\n",
        "\n",
        "        if len(chunk_text) > 1000:\n",
        "            # If a chunk is too big, split it recursively but keep its header metadata\n",
        "            sub_chunks = recursive_splitter.create_documents([chunk_text])\n",
        "            for sub_chunk in sub_chunks:\n",
        "                # Add the original header metadata to the new sub-chunk\n",
        "                sub_chunk.metadata = chunk.metadata.copy()\n",
        "                sub_chunk.metadata['source'] = row['url']\n",
        "                all_documents.append(sub_chunk)\n",
        "        else:\n",
        "            # If the chunk is a good size, just update its content and metadata\n",
        "            chunk.page_content = chunk_text\n",
        "            chunk.metadata['source'] = row['url']\n",
        "            all_documents.append(chunk)\n",
        "\n",
        "print(f\" Successfully created {len(all_documents)} structurally-aware documents (chunks).\")\n",
        "\n",
        "# Display a sample chunk to see the result\n",
        "if all_documents:\n",
        "    print(\"\\n--- SAMPLE CHUNK ---\")\n",
        "    sample_chunk = all_documents[5] # Using a different index for variety\n",
        "    print(sample_chunk.page_content)\n",
        "    print(f\"\\nMetadata: {sample_chunk.metadata}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uG6pDjST4dyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Knowledge base for RAG."
      ],
      "metadata": {
        "id": "QKsvEGKd6N-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-huggingface sentence-transformers chromadb -q langchain-chroma"
      ],
      "metadata": {
        "id": "yd5vOFYa7V1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Build RAG Knowledge Base (Vector Store)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document # Included for clarity\n",
        "\n",
        "print(\"\\n--- Building RAG Knowledge Base ---\")\n",
        "\n",
        "# 1. Load the Embedding Model\n",
        "# This model will convert your text chunks into numerical vectors.\n",
        "print(\"Loading embedding model (this may take a minute)...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'} # Use CPU for broad compatibility\n",
        ")\n",
        "print(\" Embedding model loaded.\")\n",
        "\n",
        "\n",
        "# 2. Create the Chroma Vector Store from the documents\n",
        "# This is the step where your chunks are embedded and stored.\n",
        "print(\"Storing chunks in the vector database...\")\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=all_documents,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./final_chroma_db\"  # This saves the database to a local folder for reuse\n",
        ")\n",
        "print(\" Vector store created.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- RAG Data Preparation Complete! ---\")\n",
        "print(\"The 'vector_store' variable now holds your complete, searchable knowledge base.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "drqGBoia6TmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding GEMNI as the Powerful Generalist Model."
      ],
      "metadata": {
        "id": "ofN8yfhCAS9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai -q"
      ],
      "metadata": {
        "id": "u-sT2-_dBMi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for Google Gemini\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\" Google API Key configured successfully.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\" Error: Secret 'GOOGLE_API_KEY' not found. Please add it to your Colab secrets.\")"
      ],
      "metadata": {
        "id": "arEzZe3ECqbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Knowledge back into the run-time"
      ],
      "metadata": {
        "id": "xP7PG74hDxye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Existing Vector Database\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "print(\"--- Loading the Knowledge Base from Disk ---\")\n",
        "\n",
        "#  Initialize the same embedding model used to create the database\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "# 2. Load the saved vector store from the specified directory\n",
        "persist_directory = \"./final_chroma_db\"\n",
        "vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "print(\" Knowledge Base loaded successfully!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xHgLNvySC9rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and Build RAG Engine"
      ],
      "metadata": {
        "id": "yGRMrloIGq1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Stuffing File."
      ],
      "metadata": {
        "id": "w0cIgd6TIblp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create the Structured JSON Prompt Files\n",
        "import json\n",
        "\n",
        "# 1. Define the prompt structure for the history-aware retriever\n",
        "contextualize_q_prompt_data = [\n",
        "    (\"system\", \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"),\n",
        "    (\"placeholder\", \"{chat_history}\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "]\n",
        "\n",
        "# 2. Define the prompt structure for the final QA chain\n",
        "qa_prompt_data = [\n",
        "    [\n",
        "        \"system\",\n",
        "        \"You are an expert assistant for the House of Shipping company. Your tone should be professional and helpful. Answer the user's latest question using both the chat history and the retrieved context.\\n\\n**Rules:**\\n- Synthesize information from both the 'Retrieved Context' and the 'Chat History' to form a complete, conversational answer.\\n- If the 'Retrieved Context' does not contain information to answer the latest question, you MUST state: \\\"I do not have enough information to answer that specific question.\\\"\\n- Do not contradict information that has already been established in the 'Chat History'.\\n\\n**Retrieved Context:**\\n{context}\\n\"\n",
        "    ],\n",
        "    [\n",
        "        \"placeholder\",\n",
        "        \"{chat_history}\"\n",
        "    ],\n",
        "    [\n",
        "        \"human\",\n",
        "        \"{input}\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "# 3. Write the structures to JSON files\n",
        "with open(\"contextualize_prompt.json\", \"w\") as f:\n",
        "    json.dump(contextualize_q_prompt_data, f, indent=2)\n",
        "\n",
        "with open(\"qa_prompt.json\", \"w\") as f:\n",
        "    json.dump(qa_prompt_data, f, indent=2)\n",
        "\n",
        "print(\"âœ… Structured prompt files 'contextualize_prompt.json' and 'qa_prompt.json' created.\")"
      ],
      "metadata": {
        "id": "80rMIItbQtQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# History-Aware Retriever"
      ],
      "metadata": {
        "id": "GWBFqTPxQ7Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "# In a real app, you would have one of these for each user session\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "print(\"âœ… Chat history object initialized.\")"
      ],
      "metadata": {
        "id": "svrIvNPdT2JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# 1. Create the Retriever from your loaded vector store\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# 2. Define the Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "# 3. Define the helper function to format documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "print(\"âœ… Retriever, LLM, and helper functions are ready.\")"
      ],
      "metadata": {
        "id": "OnKwKQSvT-GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Load the structured prompt from its JSON file\n",
        "with open(\"contextualize_prompt.json\", \"r\") as f:\n",
        "    contextualize_q_prompt_data = json.load(f)\n",
        "\n",
        "\n",
        "# Manually build the list of messages, creating an explicit MessagesPlaceholder object.\n",
        "messages = []\n",
        "for role, content in contextualize_q_prompt_data:\n",
        "    if role == \"placeholder\":\n",
        "        # Extract the variable name from inside the curly braces (e.g., \"{chat_history}\")\n",
        "        variable_name = content.strip('{}')\n",
        "        messages.append(MessagesPlaceholder(variable_name=variable_name))\n",
        "    else:\n",
        "        messages.append((role, content))\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "# --------------------\n",
        "\n",
        "# Chaining\n",
        "history_aware_retriever_chain = (\n",
        "    contextualize_q_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | retriever\n",
        ")\n",
        "\n",
        "print(\"âœ… History-aware retriever chain created successfully.\")"
      ],
      "metadata": {
        "id": "hQ3M3p0vUHTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Load the structured prompt for the final answer from its JSON file\n",
        "with open(\"qa_prompt.json\", \"r\") as f:\n",
        "    qa_prompt_data = json.load(f)\n",
        "\n",
        "# Manually build the list of messages, creating an explicit MessagesPlaceholder object.\n",
        "messages = []\n",
        "for item in qa_prompt_data:\n",
        "    role, content = item\n",
        "    if role == \"placeholder\":\n",
        "        variable_name = content.strip('{}')\n",
        "        messages.append(MessagesPlaceholder(variable_name=variable_name))\n",
        "    else:\n",
        "        messages.append((role, content))\n",
        "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "\n",
        "def retrieve_and_format_context(inputs):\n",
        "    # First, invoke the history-aware retriever to get the documents\n",
        "    docs = history_aware_retriever_chain.invoke({\n",
        "        \"input\": inputs[\"input\"],\n",
        "        \"chat_history\": inputs[\"chat_history\"]\n",
        "    })\n",
        "    # Then, format the documents into a single string\n",
        "    return format_docs(docs)\n",
        "# --------------------\n",
        "\n",
        "\n",
        "# Build the final conversational RAG chain using the corrected logic\n",
        "conversational_rag_chain = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"context\": retrieve_and_format_context\n",
        "    }\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\" Final conversational RAG chain is built and ready for testing.\")"
      ],
      "metadata": {
        "id": "Ogm8fncoUsMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "blcil9lfKHJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Comprehensive Testing Script for the RAG Agent\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the test suite as a list of test cases\n",
        "# Each test case has a name, a type, and a sequence of questions (steps)\n",
        "test_suite = [\n",
        "    {\n",
        "        \"name\": \"Fact Retrieval about Services\",\n",
        "        \"type\": \"Knowledge\",\n",
        "        \"steps\": [\"What kind of IT services are offered by House of Shipping?\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Conceptual Question from Insights\",\n",
        "        \"type\": \"Knowledge\",\n",
        "        \"steps\": [\"Based on your articles, what is the importance of digital transformation for the shipping industry?\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Rule-Following - Out of Scope Question\",\n",
        "        \"type\": \"Rule-Following\",\n",
        "        \"steps\": [f\"What is the current time in Lucknow? It is currently {datetime.now().strftime('%I:%M:%S %p')}.\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Conversational Memory - Simple Follow-up\",\n",
        "        \"type\": \"Memory\",\n",
        "        \"steps\": [\n",
        "            \"How did House of Shipping assist WeFreight with their expansion?\",\n",
        "            \"What was the direct result of that assistance?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Conversational Memory - Topic Change\",\n",
        "        \"type\": \"Memory\",\n",
        "        \"steps\": [\n",
        "            \"Tell me about the company's legal services.\",\n",
        "            \"Does that also include any marketing services?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Rule-Following - Plausible but Unavailable Detail\",\n",
        "        \"type\": \"Rule-Following\",\n",
        "        \"steps\": [\"Can you give me the direct phone number for the Global CEO, Alessandra Ronco?\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Test Runner ---\n",
        "print(\" STARTING COMPREHENSIVE AGENT TEST SUITE ðŸš€\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, test in enumerate(test_suite):\n",
        "    print(f\"\\n EXECUTING TEST {i+1}: {test['name']} ({test['type']})\")\n",
        "    print(\"-\"*40)\n",
        "\n",
        "    # Initialize a fresh, independent chat history for each test case\n",
        "    test_chat_history = ChatMessageHistory()\n",
        "\n",
        "    # Loop through the steps (questions) for the current test\n",
        "    for step, question in enumerate(test['steps']):\n",
        "        print(f\"\\nStep {step + 1} of {len(test['steps'])}\")\n",
        "        print(f\" You: {question}\")\n",
        "\n",
        "        # Invoke the conversational chain with the current question and the test's history\n",
        "        answer = conversational_rag_chain.invoke({\n",
        "            \"input\": question,\n",
        "            \"chat_history\": test_chat_history.messages\n",
        "        })\n",
        "\n",
        "        print(f\"\\nðŸ¤– AI: {answer}\")\n",
        "\n",
        "        # Update the test's chat history for the next step\n",
        "        test_chat_history.add_user_message(question)\n",
        "        test_chat_history.add_ai_message(answer)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\n TEST SUITE COMPLETE .... Shakalaka BOOM BOOM\")"
      ],
      "metadata": {
        "id": "_UwQ5dMFU7nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now Improve the Prompt Rules for Handling User more Efficiently."
      ],
      "metadata": {
        "id": "qmo3aaqGKV6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And If needed simply Ingest more DATA to it and keep the Agent Up-to Date or simply add cron jobs to scrape the site after specific time Interval."
      ],
      "metadata": {
        "id": "QImmBUaOKpo3"
      }
    }
  ]
}